# MLEM Documentation

**MLEM** makes it easy to deploy and serve your Machine Learning models.
It packages your models in a standardized format, and provides a unified
deployment & serving interface which supports a variety of scenarios like real-time 
serving and batch processing.

Effortlessly turn your models into python packages, docker
images or HTTP REST services, and deploy them locally or to cloud infrastructure.

> 💡 When combined with [GTO](https://github.com/iterative/gto), MLEM allows you to create
> a powerful Model Registry out of your Git repository!
> Such a registry serves as a centralized place to store and operationalize your models
> along with their metadata; manage model life-cycle, versions & releases, and easily 
> automate tests and deployments using GitOps.

<cards>

  <card href="/doc/get-started" heading="Get Started">
    A step-by-step introduction into basic MLEM features
  </card>

  <card href="/doc/user-guide" heading="User Guide">
    Study the detailed inner-workings of MLEM in its user guide.
  </card>

  <card href="/doc/use-cases" heading="Use Cases">
    Non-exhaustive list of scenarios MLEM can help with
  </card>

  <card href="/doc/api-reference" heading="API Reference">
    See all of MLEM's commands.
  </card>

</cards>

✅ Please join our [community](/community) or use the [support](/support)
channels if you have any questions or need specific help. We are very responsive
⚡.

✅ Check out our [GitHub repository](https://github.com/iterative/mlem) and give
us a ⭐ if you like the project!

✅ Contribute to MLEM [on GitHub](https://github.com/iterative/mlem) or help us
improve this [documentation](https://github.com/iterative/mlem.ai) 🙏.
